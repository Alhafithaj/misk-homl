---
title: "K-nearest Neighbor"
author: "Misk Academy"
date: "2020-6-22"
output:
  xaringan::moon_reader:
    css: ["custom.css"]
    self_contained: false
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false 
---

```{r setup, include=FALSE, cache=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center", 
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)
library(tidyverse)
# set ggplot to black and white theme
library(ggplot2)
theme_set(theme_bw())
```

class: center, middle, inverse

.font300.white[K-nearest Neighbor]

---
# Prerequisites

.pull-left[

```{r knn-pkgs, message=FALSE}
# Helper packages
library(dplyr)      # for data wrangling
library(ggplot2)    # for awesome graphics
library(rsample)    # for creating validation splits
library(recipes)    # for feature engineering

# Modeling packages
library(caret)       # for fitting KNN models
```

]

.pull-right[

```{r knn-data-prereq}
# Ames housing data
ames <- AmesHousing::make_ames()
set.seed(123)
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)

# create training (70%) set for the rsample::attrition data.
attrit <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)
set.seed(123)
churn_split <- initial_split(attrit, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)

# import MNIST training data
mnist <- dslabs::read_mnist()
names(mnist)
```

]

---
# Measuring similarity

.pull-left[

* KNN algorithm identifies $k$ observations that are "similar" or nearest to the new record being predicted

* Real estate analogy --> determine what price they will list (or market) a home for based on "comps" (comparable homes)

* Homes that have very similar attributes to the one being sold (e.g., square footage, number of rooms, style of the home, neighborhood and school district)

]

.pull-right[

```{r map-homes, echo=FALSE, fig.width=7, fig.height=8}
library(leaflet)

df <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  prep(training = ames_train, retain = TRUE) %>%
  juice() %>%
  select(-Sale_Price)

home <- 30
k = 11
index <- as.vector(FNN::knnx.index(df[-home, ], df[home, ], k = k))
sale_home <- ames_train[home, ] %>%
  mutate(type = "target")
like_homes <- ames_train[index, ] %>%
  mutate(type = "like")
knn_homes <- rbind(sale_home, like_homes)

pal <- colorFactor(c("blue", "red"),
                   domain = unique(knn_homes$type))

knn_homes %>%
  filter(Neighborhood != "Stone_Brook") %>%
  leaflet() %>%
  addTiles() %>%
  addCircleMarkers(lng = ~Longitude,
                   lat = ~Latitude,
                   stroke = FALSE,
                   color = ~pal(type),
                   fillOpacity = .75
                   )
```


]

---
# Distance measures

How do we determine the similarity between observations (or homes as in the previous example)?

.pull-left[


\begin{equation}
 \text{Euclidean: }\sqrt{\sum^P_{j=1}(x_{aj} - x_{bj})^2}
\end{equation}

]

.pull-right[

\begin{equation}
 \text{Manhattan: }\sum^P_{j=1} | x_{aj} - x_{bj} | 
\end{equation}

]

To illustrate let's look at two homes:

```{r}
(two_houses <- ames_train[1:2, c("Gr_Liv_Area", "Year_Built")])
```

---
# Distance measures

How do we determine the similarity between observations (or homes as in the previous example)?

.pull-left[


\begin{equation}
 \text{Euclidean: }\sqrt{\sum^P_{j=1}(x_{aj} - x_{bj})^2}
\end{equation}

```{r}
dist(two_houses, method = "euclidean")
```

```{r, echo=FALSE, fig.height=3.5}
ggplot(two_houses, aes(Gr_Liv_Area, Year_Built)) +
  geom_point() +
  geom_line(lty = "dashed") +
  ggtitle("(A) Euclidean distance")
```


]

.pull-right[

\begin{equation}
 \text{Manhattan: }\sum^P_{j=1} | x_{aj} - x_{bj} | 
\end{equation}

```{r}
dist(two_houses, method = "manhattan")
```

```{r, echo=FALSE,  fig.height=3.5}
ggplot(two_houses, aes(Gr_Liv_Area, Year_Built)) +
  geom_point() +
  geom_step(lty = "dashed") +
  ggtitle("(B) Manhattan distance")
```

]

---
# Pre-processing

.pull-left[

* Due to the squaring in the Euclidean distance function, the Euclidean distance is more sensitive to outliers. 
* Furthermore, most distance measures are sensitive to the scale of the features. 
* Data with features that have different scales will bias the distance measures as those predictors with the largest values will contribute most to the distance between two samples.  
* For example, consider the three home below: `home1` is a four bedroom built in 2008, `home2` is a two bedroom built in the same year, and `home3` is a three bedroom built a decade earlier.
]

.pull-right[

```{r scale-impacts-distance-hidden, echo=FALSE}
home1 <- ames %>%
  mutate(id = row_number()) %>%
  select(Bedroom_AbvGr, Year_Built, id) %>%
  filter(Bedroom_AbvGr == 4 & Year_Built == 2008) %>%
  dplyr::slice(1) %>%
  mutate(home = "home1") %>%
  select(home, everything())

home2 <- ames %>%
  mutate(id = row_number()) %>%
  select(Bedroom_AbvGr, Year_Built, id) %>%
  filter(Bedroom_AbvGr == 2 & Year_Built == 2008) %>%
  dplyr::slice(1) %>%
  mutate(home = "home2") %>%
  select(home, everything())

home3 <- ames %>%
  mutate(id = row_number()) %>%
  select(Bedroom_AbvGr, Year_Built, id) %>%
  filter(Bedroom_AbvGr == 3 & Year_Built == 1998) %>%
  dplyr::slice(1) %>%
  mutate(home = "home3") %>%
  select(home, everything())
```

```{r}
home1
home2
home3
```

]

---
# Pre-processing

.pull-left[

* Due to the squaring in the Euclidean distance function, the Euclidean distance is more sensitive to outliers. 
* Furthermore, most distance measures are sensitive to the scale of the features. 
* Data with features that have different scales will bias the distance measures as those predictors with the largest values will contribute most to the distance between two samples.  
* For example, consider the three home below: `home1` is a four bedroom built in 2008, `home2` is a two bedroom built in the same year, and `home3` is a three bedroom built a decade earlier.

]

.pull-right[

```{r scale-impacts-distance2}
features <- c("Bedroom_AbvGr", "Year_Built")

# distance between home 1 and 2
dist(rbind(home1[,features], home2[,features]))

# distance between home 1 and 3
dist(rbind(home1[,features], home3[,features]))
```

]

<br>

.center.bold[The Euclidean distance between `home1` and `home3` is larger due to the larger difference in `Year_Built` with `home2`.]

---
# Pre-processing

.scrollable90[
.pull-left[

* Due to the squaring in the Euclidean distance function, the Euclidean distance is more sensitive to outliers. 
* Furthermore, most distance measures are sensitive to the scale of the features. 
* Data with features that have different scales will bias the distance measures as those predictors with the largest values will contribute most to the distance between two samples.  
* For example, consider the three home below: `home1` is a four bedroom built in 2008, `home2` is a two bedroom built in the same year, and `home3` is a three bedroom built a decade earlier.
* .bold[Standardizing eliminates these magnitude differences.]

]

.pull-right[

```{r scaling, echo=FALSE}
scaled_ames <- recipe(Sale_Price ~ ., ames_train) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  prep(training = ames, retain = TRUE) %>%
  juice()

home1_std <- scaled_ames %>%
  mutate(id = row_number()) %>%
  filter(id == home1$id) %>%
  select(Bedroom_AbvGr, Year_Built, id) %>%
  mutate(home = "home1") %>%
  select(home, everything())

home2_std <- scaled_ames %>%
  mutate(id = row_number()) %>%
  filter(id == home2$id) %>%
  select(Bedroom_AbvGr, Year_Built, id) %>%
  mutate(home = "home2") %>%
  select(home, everything())

home3_std <- scaled_ames %>%
  mutate(id = row_number()) %>%
  filter(id == home3$id) %>%
  select(Bedroom_AbvGr, Year_Built, id) %>%
  mutate(home = "home3") %>%
  select(home, everything())
```

```{r scale-impacts-distance3}
home1_std
home2_std
home3_std

# distance between home 1 and 2
dist(rbind(home1_std[,features], home2_std[,features]))

# distance between home 1 and 3
dist(rbind(home1_std[,features], home3_std[,features]))
```

]]

---
# Choosing K

.scrollable90[
.pull-left[

* $k$ is our one hyperparameter!
* When $k = 1$, we base our prediction on a single observation that has the closest distance measure.
* When $k = n$, we are simply using the average (regression) or most common class (classification) across all training samples as our predicted value.
* No general rule about the best $k$ as it depends greatly on the nature of the data. 
* For high signal data with very few noisy (irrelevant) features, smaller values of $k$ tend to work best. As more irrelevant features are involved, larger values of $k$ are required to smooth out the noise
* .bold[Pro Tip]: When using KNN for classification, it is best to assess odd numbers for $k$ to avoid ties in the event there is equal proportion of response levels.

]

.pull-right[

```{r range-k-values, fig.height=6}
# Create blueprint
blueprint <- recipe(Attrition ~ ., data = churn_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(contains("Satisfaction")) %>%
  step_integer(WorkLifeBalance) %>%
  step_integer(JobInvolvement) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())

# Create a resampling method
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5,
  classProbs = TRUE,                 
  summaryFunction = twoClassSummary
)

# Create a hyperparameter grid search
hyper_grid <- expand.grid(
  k = floor(seq(1, nrow(churn_train)/3, length.out = 20))
)

# Fit knn model and perform grid search
knn_grid <- train(
  blueprint, 
  data = churn_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "ROC"
)

ggplot(knn_grid)
```

]]

---
# MNIST

.pull-left[

* Due to size let's just take a subset

]

.pull-right[

```{r}
set.seed(123)
index <- sample(nrow(mnist$train$images), size = 10000)
mnist_x <- mnist$train$images[index, ]
mnist_y <- factor(mnist$train$labels[index])
```


]

---
# MNIST

.pull-left[

* Due to size let's just take a subset
* Lots of near-zero variance features

]

.pull-right[

```{r, fig.height=4.5}
mnist_x %>%
  as.data.frame() %>%
  purrr::map_df(sd) %>%
  gather(feature, sd) %>%
  ggplot(aes(sd)) +
  geom_histogram(binwidth = 1)
```


]

---
# MNIST

.pull-left[

* Due to size let's just take a subset
* Lots of near-zero variance features

]

.pull-right[

```{r mnist-plot-nzv-feature-image, echo=FALSE, fig.width=8, fig.height=4.5, fig.cap="Example images (A)-(C) from our data set and (D) highlights near-zero variance features around the edges of our images."}
nzv <- nearZeroVar(mnist_x)
par(mfrow = c(1, 4))
i <- 2
image(1:28, 1:28, matrix(mnist$test$images[i,], nrow=28)[ , 28:1], 
      col = gray(seq(0, 1, 0.05)), xlab = "", ylab="", 
      xaxt="n", yaxt="n", main = "(A) Example image \nfor digit 2")
i <- 7
image(1:28, 1:28, matrix(mnist$test$images[i,], nrow=28)[ , 28:1], 
      col = gray(seq(0, 1, 0.05)), xlab = "", ylab="", 
      xaxt="n", yaxt="n", main = "(B) Example image \nfor digit 4")
i <- 9
image(1:28, 1:28, matrix(mnist$test$images[i,], nrow=28)[ , 28:1], 
      col = gray(seq(0, 1, 0.05)), xlab = "", ylab="", 
      xaxt="n", yaxt="n", main = "(C) Example image \nfor digit 5")
image(matrix(!(1:784 %in% nzv), 28, 28), col = gray(seq(0, 1, 0.05)), 
      xaxt="n", yaxt="n", main = "(D) Typical variability \nin images.")
```

]

---
# MNIST

.pull-left[

* Due to size let's just take a subset
* Lots of near-zero variance features
* Removing these zero (or near-zero) variance features, we end up keeping 46 of the original 249 predictors
   - can cause dramatic improvements to both the accuracy and speed of our algorithm
]

.pull-right[

```{r}
# Rename features
colnames(mnist_x) <- paste0("V", 1:ncol(mnist_x))

# Remove near zero variance features manually
nzv <- nearZeroVar(mnist_x)
index <- setdiff(1:ncol(mnist_x), nzv)
mnist_x <- mnist_x[, index]
```

]

---
# MNIST

.scrollable90[
.pull-left[

* Due to size let's just take a subset
* Lots of near-zero variance features
* Removing these zero (or near-zero) variance features, we end up keeping 46 of the original 249 predictors
   - can cause dramatic improvements to both the accuracy and speed of our algorithm
* .bold.red[Warning]: Our hyperparameter grid search assesses 13 k values between 1–25 and takes approximately 3 minutes.

]

.pull-right[

```{r mnist-initial-model, fig.height=4, fig.cap="KNN search grid results for the MNIST data"}
# Use train/validate resampling method
cv <- trainControl(
  method = "LGOCV", 
  p = 0.7,
  number = 1,
  savePredictions = TRUE
)

# Create a hyperparameter grid search
hyper_grid <- expand.grid(k = seq(3, 25, by = 2))

# Execute grid search
knn_mnist <- train(
  mnist_x,
  mnist_y,
  method = "knn",
  tuneGrid = hyper_grid,
  preProc = c("center", "scale"),
  trControl = cv
)

ggplot(knn_mnist)
```

]]

---
# Results

.pull-left[

* 94% accuracy rate

* hardest to detect
   - 8s
   - 4s
   - 3s
   - 2s

]

.pull-right[

```{r}
# Create confusion matrix
cm <- confusionMatrix(knn_mnist$pred$pred, knn_mnist$pred$obs)
cm$byClass[, c(1:2, 11)]  # sensitivity, specificity, & accuracy
```

]

---
# Visualizing correct & incorrect predictions

.scrollable90[

```{r correct-vs-incorrect, fig.height=10, fig.width=12, fig.cap="Actual images from the MNIST data set along with our KNN model's predictions.  Left column illustrates a few accurate predictions and the right column illustrates a few inaccurate predictions."}
# Get a few accurate predictions
set.seed(9)
good <- knn_mnist$pred %>%
  filter(pred == obs) %>%
  sample_n(4)

# Get a few inaccurate predictions
set.seed(9)
bad <- knn_mnist$pred %>%
  filter(pred != obs) %>%
  sample_n(4)

combine <- bind_rows(good, bad)

# Get original feature set with all pixel features
set.seed(123)
index <- sample(nrow(mnist$train$images), 10000)
X <- mnist$train$images[index,]

# Plot results
par(mfrow = c(4, 2), mar=c(1, 1, 1, 1))
layout(matrix(seq_len(nrow(combine)), 4, 2, byrow = FALSE))
for(i in seq_len(nrow(combine))) {
  image(matrix(X[combine$rowIndex[i],], 28, 28)[, 28:1], 
        col = gray(seq(0, 1, 0.05)),
        main = paste("Actual:", combine$obs[i], "  ", 
                     "Predicted:", combine$pred[i]),
        xaxt="n", yaxt="n") 
}
```

]

---
# Summary

- KNNs are a very simplistic, and intuitive, algorithm that can provide average to decent predictive power, especially when the response is dependent on the local structure of the features

- Major drawback of KNNs is their computation time, which increases by $n \times p$ for each observation

- Furthermore, since KNNs are a lazy learner, they require the model be run at prediction time which limits their use for real-time modeling

- Although KNNs rarely provide the best predictive performance, they have many benefits, for example, in feature engineering and in data cleaning and preprocessing

---
class: clear, center, middle

background-image: url(images/any-questions.jpg)
background-position: center
background-size: cover

---
# Back home

<br><br><br><br>
[.center[`r anicon::faa("home", size = 10, animate = FALSE)`]](https://github.com/misk-data-science/misk-homl)

.center[https://github.com/misk-data-science/misk-homl]
