---
title: "Linear Regression & Cousins"
author: "Misk Academy"
date: "2020-6-22"
output:
  xaringan::moon_reader:
    css: ["custom.css"]
    self_contained: false
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false  
---

```{r setup, include=FALSE, cache=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center", 
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)
library(tidyverse)
# set ggplot to black and white theme
library(ggplot2)
theme_set(theme_bw())
```

class: clear, center, middle

background-image: url(https://educationalresearchtechniques.files.wordpress.com/2014/08/1.jpg?w=624)
background-position: center
background-size: cover

<br><br><br><br><br><br><br><br><br><br><br><br><br>
.font200.bold[Regression & Cousins]

---
# Introduction

.pull-left[

.center.bold.font120[Thoughts]

- a fundamental analytic method
- still widely used
- basic approaches have large assumptions
- serves as a foundation to many extension methods

]

--

.pull-right[

.center.bold.font120[Overview]

- Ordinary Least Squares
- Principal Component Regression
- Partial Least Squares Regression

]

---
# Prereqs .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 1]

.pull-left[

.center.bold.font120[Packages]

```{r prereqs-pks}
library(dplyr)
library(ggplot2)
library(rsample)
library(recipes)
library(vip)
library(caret)
```


]

.pull-right[

.center.bold.font120[Data]

```{r prereqs-data}
# ames data
ames <- AmesHousing::make_ames()

# split data
set.seed(123)
split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(split)
```

]

---
# The Objective

```{r, echo=FALSE, fig.height=5.5, fig.width=11}
lm(Sale_Price ~ Gr_Liv_Area, data = ames_train) %>%
  broom::augment() %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
  geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
                   xend = Gr_Liv_Area, yend = .fitted), 
               alpha = 0.3) +
  geom_point(size = 2, alpha = 0.2) +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("Fitted regression line (with residuals)")
```


* Model form: $y_i = \beta_0 + \beta_{1}x_{i1} + \beta_{2}x_{i2} \cdots + \beta_{p}x_{ip} + \epsilon_i$

* Objective function: $\text{minimize} \bigg \{ SSE = \sum^n_{i=1} (y_i - \hat{y}_i)^2 \bigg \} \equiv \text{minimize MSE}$

---
# Simple linear regression .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 2]

.pull-left.font120[

- .bold.blue[`lm()`] performs OLS in base R

- `glm()` also performs linear regression but extends to other generalized methods (i.e. logistic regression)

- `summary(model)` provides many results (i.e. "Residual Standard Error" is the RMSE)

- No method for resampling (i.e. cross validation) with `lm()`

]

.pull-right[
```{r}
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
summary(model1)
```
]

---
# Multiple linear regression .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 3]

.pull-left[

```{r}
# OLS model with two predictors
model2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)

# OLS model with specified interactions
model3 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area : Year_Built, data = ames_train)

# include all possible main effects
model4 <- lm(Sale_Price ~ ., data = ames_train)
```


]

.pull-right[
```{r, echo=FALSE}
library(plotly)
library(reshape2)

# model
model2 <- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)

# Setup Axis
axis_x <- seq(min(ames_train$Gr_Liv_Area), max(ames_train$Gr_Liv_Area), by = 50)
axis_y <- seq(min(ames_train$Year_Built), max(ames_train$Year_Built), by = 10)

# Sample points
lm_surface <- expand.grid(Gr_Liv_Area = axis_x, Year_Built = axis_y, KEEP.OUT.ATTRS = F)
lm_surface$Sale_Price <- predict.lm(model2, newdata = lm_surface)
lm_surface <- acast(lm_surface, Year_Built ~ Gr_Liv_Area, value.var = "Sale_Price")

# plot
ames_plot <- plot_ly(ames_train,
                     x = ~ Gr_Liv_Area, 
                     y = ~ Year_Built, 
                     z = ~ Sale_Price,
                     type = "scatter3d", 
                     mode = "markers",
                     marker = list(
                       size = 5,
                       opacity = 0.25
                     ),
                     showlegend = F
                     )

# add surface
ames_plot <- add_trace(p = ames_plot,
                       z = lm_surface,
                       x = axis_x,
                       y = axis_y,
                       type = "surface")
ames_plot
```

]

---
# Assessing model accuracy

.pull-left[

We've fit four models to the Ames housing data: 

1. a single predictor, 
2. two predictors, 
3. two predictors with interaction,
4. and all possible main effect predictors. 

<br>

.center.bold.blue[Which model is "best"?]

]

---
# Assessing model accuracy .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 4]

.scrollable90[
.pull-left[

We've fit four models to the Ames housing data: 

1. a single predictor, 
2. two predictors, 
3. two predictors with interaction,
4. and all possible main effect predictors. 

<br>

.center.bold.blue[Which model is "best"?]

]

.pull-right[

```{r}
# create a resampling method
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
  )

# model 1 CV
set.seed(123)
(cv_model1 <- train(
  Sale_Price ~ Gr_Liv_Area, 
  data = ames_train, 
  method = "lm", #<<
  trControl = cv)
)
```

]
]

---
# Assessing model accuracy .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 5]

.scrollable90[
.pull-left[

We've fit four models to the Ames housing data: 

1. a single predictor, 
2. two predictors, 
3. two predictors with interaction,
4. and all possible main effect predictors. 

<br>

.center.bold.blue[Model using most predictors is marginally superior]

]

.pull-right[

```{r}
# model 2 CV
set.seed(123)
cv_model2 <- train(
  Sale_Price ~ Gr_Liv_Area + Year_Built, 
  data = ames_train, 
  method = "lm",
  trControl = cv
  )

# model 3 CV
set.seed(123)
cv_model3 <- train(
  Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area : Year_Built, 
  data = ames_train, 
  method = "lm",
  trControl = cv
  )

# model 4 CV
set.seed(123)
cv_model4 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "lm",
  trControl = cv
  )

# Extract out of sample performance measures
summary(resamples(list(
  model1 = cv_model1, 
  model2 = cv_model2, 
  model3 = cv_model3,
  model4 = cv_model4
)))
```

]
]

---
# Model concerns

.pull-left[

<br><br><br>

.bold.center[With simplistic models comes many assumptions...often at the expense of model performance]

]

.pull-right[

<br>

```{r concerns-png, echo=FALSE}
knitr::include_graphics("https://media1.tenor.com/images/3c888132eb6fbedec9a131bc55a05315/tenor.gif?itemid=10744949")
```


]

---
# Model concerns

.pull-left[
1. .bold.red[Linear relationship]
2. Constant variance among residuals
3. No autocorrelation
4. More observations than predictors
5. No or little multicollinearity

<br>

.bold.center[<u>Sometimes</u> we can resolve this with transformations]

]

.pull-right[

```{r, echo=FALSE}
p1 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) + 
  geom_point(size = 1, alpha = .4) +
  geom_smooth(se = FALSE) +
  scale_y_continuous("Sale price", labels = scales::dollar) +
  xlab("Year built") +
  ggtitle("Non-transformed variables with a \nnon-linear relationship.")

p2 <- ggplot(ames_train, aes(Year_Built, Sale_Price)) + 
  geom_point(size = 1, alpha = .4) + 
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_log10("Sale price", labels = scales::dollar, breaks = seq(0, 400000, by = 100000)) +
  xlab("Year built") +
  ggtitle("Transforming variables can provide a \nnear-linear relationship.")

gridExtra::grid.arrange(p1, p2, nrow = 2)
```


]

---
# Model concerns

.pull-left[
1. Linear relationship
2. .bold.red[Constant variance among residuals]
3. No autocorrelation
4. More observations than predictors
5. No or little multicollinearity

<br>

.bold.center[<u>Sometimes</u> we can resolve this with transformations or adding more features]
]

.pull-right[

```{r, echo=FALSE}
df1 <- broom::augment(cv_model1$finalModel, data = ames_train)

p1 <- ggplot(df1, aes(.fitted, .resid)) + 
  geom_point(size = 1, alpha = .4) +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Model 1",
    subtitle = "Sale_Price ~ Gr_Liv_Area")

df2 <- broom::augment(cv_model3$finalModel, data = ames_train)

p2 <- ggplot(df2, aes(.fitted, .resid)) + 
  geom_point(size = 1, alpha = .4)  +
  xlab("Predicted values") +
  ylab("Residuals") +
  ggtitle("Model 3",
    subtitle = "Sale_Price ~ .")

gridExtra::grid.arrange(p1, p2, nrow = 2)
```

]

---
# Model concerns

.pull-left[
1. Linear relationship
2. Constant variance among residuals
3. .bold.red[No autocorrelation]
4. More observations than predictors
5. No or little multicollinearity

<br>

.bold.center[<u>Sometimes</u> we can resolve this by adding more features]
]

.pull-right[

```{r, echo=FALSE}
df1 <- mutate(df1, id = row_number())
df2 <- mutate(df2, id = row_number())

p1 <- ggplot(df1, aes(id, .resid)) + 
  geom_point(size = 1, alpha = .4) +
  xlab("Row ID") +
  ylab("Residuals") +
  ggtitle("Model 1",
    subtitle = "Correlated residuals.") +
  geom_smooth(se = FALSE, span = .2)

p2 <- ggplot(df2, aes(id, .resid)) + 
  geom_point(size = 1, alpha = .4) +
  xlab("Row ID") +
  ylab("Residuals") +
  ggtitle("Model 3",
    subtitle = "Uncorrelated residuals.") +
  geom_smooth(se = FALSE, span = .2)

gridExtra::grid.arrange(p1, p2, nrow = 2)
```

]

---
# Model concerns

.pull-left[
1. Linear relationship
2. Constant variance among residuals
3. No autocorrelation
4. .bold.red[More observations than predictors]
5. No or little multicollinearity

<br>

.bold.center[<u>Sometimes</u> we can resolve this with feature reduction techniques]
]

.pull-right[

```{r, echo=FALSE}
data.frame(
  y = sample(100000:400000, 5, replace = TRUE), 
  x1 = sample(1:10, 5, replace = TRUE),
  x2 = sample(1:10, 5, replace = TRUE),
  x3 = sample(1:10, 5, replace = TRUE),
  x4 = sample(1:10, 5, replace = TRUE),
  x5 = sample(1:10, 5, replace = TRUE),
  x6 = sample(1:10, 5, replace = TRUE),
  x7 = sample(1:10, 5, replace = TRUE),
  x8 = sample(1:10, 5, replace = TRUE),
  x9 = sample(1:10, 5, replace = TRUE),
  x10 = sample(1:10, 5, replace = TRUE)
  ) %>%
  knitr::kable() %>%
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

.bold.center.red[Not invertible --> solutions are non-unique meaning there are many "right" solutions for our feature coefficients!]

]

---
# Model concerns .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 6]

.pull-left[
1. Linear relationship
2. Constant variance among residuals
3. No autocorrelation
4. More observations than predictors
5. .bold.red[No or little multicollinearity]

<br>

.bold.center[<u>Sometimes</u> we can resolve this with feature reduction techniques]
]

.pull-right[

```{r multicollinearity}
m1 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)
m2 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
m3 <- lm(Sale_Price ~ TotRms_AbvGrd, data = ames_train)

coef(m1) #<<
coef(m2) #<<
coef(m3) #<<
```

]

---
# Model concerns

.pull-left[
1. Linear relationship
2. Constant variance among residuals
3. No autocorrelation
4. More observations than predictors
5. No or little multicollinearity

]

.pull-right[

```{r so-complicated, echo=FALSE}
knitr::include_graphics("http://tinderdistrict.com/wp-content/uploads/2018/06/complicated.gif")
```

]

--

<br><br>

.bold.center[Many regression extensions have been developed to deal with these concerns.]

---
class: center, middle, inverse

.font300.white[Principal Component Regression]

---
# The idea

.pull-left[

PCR performs feature reduction to help minimize impact of:

- multicollinearity (becomes a bigger concern the more predictors we have)

- when $p >> n$

Steps:

1. Reduce *p* features to *c* PCs (not guided by the response)

2. Use PCs as predictors and perform regression as usual

]

.pull-right[

```{r pcr-steps, echo=FALSE, out.height="86%", out.width="86%"}
knitr::include_graphics("images/pcr-steps.png")
```

]

---
# R packages `r emo::ji("package")`

<br>

.font130[
- Any package that implements PCA can be applied prior to modeling,

- See [multivariate task view](	https://CRAN.R-project.org/view=Multivariate
) on CRAN for options; however,...

- .bold[caret] provides and integrated `method = "pcr"` that helps to automate the tuning process
]
---
# Implementation .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 7]

.pull-left[

```{r pcr}
# 1. hypergrid
hyper_grid <- expand.grid(ncomp = seq(2, 40, by = 2))

# 2. PCR
set.seed(123)
cv_pcr <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  trControl = cv,
  method = "pcr", #<<
  preProcess = c("zv", "center", "scale"), #<<
  tuneGrid = hyper_grid, #<<
  metric = "RMSE"
  )

# model with lowest RMSE
cv_pcr$bestTune

cv_pcr$results %>%
  filter(ncomp == as.numeric(cv_pcr$bestTune))
```

]

.pull-right[
```{r pcr-plot-revised, fig.height=5}
# plot cross-validated RMSE
plot(cv_pcr)
```

.center.bold[Feature reduction with PCR improves prediction error by ~ $10K]

]

---
# Tuning .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 8]

.scrollable90[
.pull-left[

- The number of PCs is the only hyperparameter

- rule of `r anicon::cia("https://emojis.slackmojis.com/emojis/images/1511903783/3230/wiggle_thumbs_up.gif?1511903783", animate = "slow")`
   - assess 2-*p* in evenly divided segments
   - start with a few and zoom in

]

.pull-right[

```{r pcr-grid-2, fig.height=5}
# 1. hypergrid
p <- length(ames_train) - 1
hyper_grid <- expand.grid(ncomp = seq(2, 80, length.out = 10)) #<<

# 2. PCR
set.seed(123)
cv_pcr <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  trControl = cv,
  method = "pcr", 
  preProcess = c("zv", "center", "scale"), 
  tuneGrid = hyper_grid, 
  metric = "RMSE"
  )

# RMSE
cv_pcr$results %>%
  filter(ncomp == cv_pcr$bestTune$ncomp)

# plot cross-validated RMSE
plot(cv_pcr)
```

]
]

---
class: center, middle, inverse

.font300.white[Partial Least Squares Regression]

---
# The idea

.pull-left[

- A problem with PCR is that the PCs are developed independent of the response.

- PLS 
   - has similar intentions as PCR
   
   - finds PCs that maximize correlation with the response
   
   - typically results in a stronger signal between PCs and response

]

.pull-right[

```{r pls-steps, echo=FALSE, out.height="94%", out.width="94%"}
knitr::include_graphics("images/pls-steps.png")
```

]

---
# The idea

.pull-left[

- A problem with PCR is that the PCs are developed independent of the response.

- PLS 
   - has similar intentions as PCR
   
   - finds PCs that maximize correlation with the response
   
   - .bold.blue[typically results in a stronger signal between PCs and response]

]

.pull-right[

```{r pls-vs-pcr-relationship, echo=FALSE}
library(AppliedPredictiveModeling)
data(solubility)
df <- cbind(solTrainX, solTrainY)

pca_df <- recipe(solTrainY ~ ., data = df) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_pca(all_predictors()) %>%
  prep(training = df, retain = TRUE) %>%
  juice() %>%
  select(PC1, PC2, solTrainY) %>%
  rename(`PCR Component 1` = "PC1", `PCR Component 2` = "PC2") %>%  
  gather(component, value, -solTrainY)

pls_df <- recipe(solTrainY ~ ., data = df) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_pls(all_predictors(), outcome = "solTrainY") %>%
  prep(training = df, retain = TRUE) %>%
  juice() %>%
  rename(`PLS Component 1` = "PLS1", `PLS Component 2` = "PLS2") %>%
  gather(component, value, -solTrainY)

pca_df %>% 
  bind_rows(pls_df) %>%
  ggplot(aes(value, solTrainY)) +
  geom_point(alpha = .25) +
  geom_smooth(method = "lm", se = FALSE, lty = "dashed") +
  facet_wrap(~ component, scales = "free") +
  labs(x = "PC Eigenvalues", y = "Response")
  
```

]

---
# R packages `r emo::ji("package")`

.pull-left[

## [`pls`](https://cran.r-project.org/package=pls)

* **p**artial **l**east **s**quares

* Original and primary implementation of PLS

* Provides both PLS & PCR capabilities 

]
.pull-right[

## [Other pkgs](https://CRAN.R-project.org/view=Multivariate)

* `ppls`: penalized partial least squares

* `dr`: provides various dimension reduction regression options

* `plsgenomics`: provides partial least squares analyses for genomics
    
]

---
# Implementation .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 9]

.pull-left[

```{r pls}
# PLS
set.seed(123)
cv_pls <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  trControl = cv,
  method = "pls", #<<
  preProcess = c("zv", "center", "scale"),
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )

# model with lowest RMSE
cv_pls$bestTune

cv_pls$results %>%
  filter(ncomp == as.numeric(cv_pls$bestTune))
```

]

.pull-right[
```{r pls-plot, fig.height=5}
# plot cross-validated RMSE
plot(cv_pls)
```

.center.bold[Using PLS improves prediction error by an additional $500]

]

---
# Tuning

- The number of PCs is the only hyperparameter

- Will almost always require less PCs than PCR

- rule of `r anicon::cia("https://emojis.slackmojis.com/emojis/images/1511903783/3230/wiggle_thumbs_up.gif?1511903783", animate = "slow")`
   - assess 2-*p* in evenly divided segments
   - start with a few and zoom in

---
class: center, middle, inverse

.font300.white[Model Comparison]

---
# Comparing error distributions .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 10]

.pull-left[

```{r compare-percentiles}
results <- resamples(list(
  OLS  = cv_model4, 
  PCR  = cv_pcr, 
  PLS  = cv_pls
  ))

summary(results)$statistics$RMSE
```

]

.pull-right[
```{r compare-bwplot, fig.height=5}
p1 <- bwplot(results, metric = "RMSE")
p2 <- dotplot(results, metric = "RMSE")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

]

.center.bold[Student's *t*-test or a rank sum test could also be used.]

---
class: center, middle, inverse

.font300.white[Feature Interpretation]

---
# Feature importance .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 11]

.pull-left[

.center.bold.font120[vip]

* .bold[v]ariable .bold[i]mportance .bold[p]lots illustrate the influence each predictor has

* many packages have their own vip plots

* the __vip__ `r emo::ji("package")` provides a common output

* different models measure "importance" differently

* we'll review this more indepth in later modules

]

.pull-right[

```{r pls-vip, fig.height=6}
vip(cv_pls)
```

]

---
# Feature importance .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 12]

```{r all-vip, fig.height=7, fig.width=16, echo=FALSE}
p1 <- vip(cv_model4, num_features = 25) + ggtitle("OLS")
#p2 <- vip(cv_pcr, num_features = 25) + ggtitle("PCR")
p3 <- vip(cv_pls, num_features = 25) + ggtitle("PLS")

gridExtra::grid.arrange(p1, p3, nrow = 1)
```

---
# Feature effects .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 13]

.pull-left[

* feature effects measures the relationship between a feature and the target variable

* most common approach is a .bold[p]artial .bold[d]ependence .bold[p]lot

* computs the average response value when all observations use a particular value for a given feature

]

.pull-right[

```{r ols-pdp, fig.height=4}
pdp::partial(cv_model2, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot()
```

]

---
# Feature effects .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 14]

<br><br>

```{r all-pdps, , fig.height=4, fig.width=16, echo=FALSE}
p1 <- pdp::partial(cv_model2, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
  ggtitle("OLS with 2 features") +
  scale_y_continuous("Predicted Sales Price", labels = scales::dollar, limits = c(0, 600000))

p2 <- pdp::partial(cv_model4, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
  ggtitle("OLS with all features") +
  scale_y_continuous("Predicted Sales Price", labels = scales::dollar, limits = c(0, 600000))

p3 <- pdp::partial(cv_pcr, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
  ggtitle("PCR") +
  scale_y_continuous("Predicted Sales Price", labels = scales::dollar, limits = c(0, 600000))

p4 <- pdp::partial(cv_pls, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
  ggtitle("PLS") +
  scale_y_continuous("Predicted Sales Price", labels = scales::dollar, limits = c(0, 600000))

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 1)
```

---
# Feature effects .red[`r anicon::faa("hand-point-right", color = "red", animate = "horizontal")` code chunk 15]

Assess the interaction of 2 predictors:

```{r interaction-pdp, fig.height=5}
pdp::partial(cv_model2, pred.var = c("Gr_Liv_Area", "Year_Built"), grid.resolution = 10) %>% 
  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))
```


---
class: center, middle, inverse

.font300.white[Wrapping up]

---
# Summary

* Ordinary least squares
   - simple but lots of assumptions
   - typically poor predictive accuracy

* Principal Component Regression
   - minimizes multicollinearity
   - helps when $p >> n$

* Partial Least Squares
   - same benefits as PCR but
   - creates stronger signal btwn PCs and target

---
class: clear, center, middle

background-image: url(images/any-questions.jpg)
background-position: center
background-size: cover

---
# Back home

<br><br><br><br>
[.center[`r anicon::faa("home", size = 10, animate = FALSE)`]](https://github.com/misk-data-science/misk-homl)

.center[https://github.com/misk-data-science/misk-homl]
