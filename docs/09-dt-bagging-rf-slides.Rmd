---
title: "Decision Trees, Bagging, & Random Forests"
author: "Misk Academy"
date: "2020-6-22"
output:
  xaringan::moon_reader:
    css: ["custom.css"]
    self_contained: false
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false  
---

```{r setup, include=FALSE, cache=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center", 
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)
library(tidyverse)
library(gganimate)
library(MASS)
library(randomForest)
library(ranger)
# set ggplot to black and white theme
library(ggplot2)
theme_set(theme_bw())
```

class: clear, center, middle

background-image: url(images/rf-icon.jpg)
background-position: center
background-size: cover

<br><br><br>
.font300.white[Decision Trees, Bagging, <br>& Random Forests]

---
# Introduction

.pull-left[

.center.bold.font120[Thoughts]

Random forests are an ensemble method that:

* provide competitive accuracy

* provide good _out-of-the-box_ performance

* capture non-linear, non-monotonic relationships

* automatically capture interactions

* easy to tune (relatively speaking)

* require minimal, if any, pre-processing

]

--

.pull-right[

.center.bold.font120[Overview]

* Provide solid understanding of:
   1. decision trees
   2. bagging
   3. random forests
   
* Apply a random forest model   

]

---
class: clear, center, middle

background-image: url(images/single-tree.gif)
background-size: cover

.font300.white[Decision Trees]

???

Image credit: [giphy](https://giphy.com/gifs/tree-U85Z0lxOwDoys?utm_source=media-link&utm_medium=landing&utm_campaign=Media%20Links&utm_term=)

---

# Basic Idea


```{r, echo=FALSE, out.height='90%', out.width='90%'}
knitr::include_graphics("images/dt-01.png")
```

.center[.content-box-gray[.bold[Will a customer redeem a coupon]]]

---

# A .red[ruleset] model

```{r, echo=FALSE, out.height='90%', out.width='90%'}
knitr::include_graphics("images/dt-03.png")
```

.font90[`if Loyal Customer = Yes and Household income >= $150K and Shopping mode = store then coupon redemption = Yes`]


---

# Terminology

```{r, echo=FALSE, out.height='90%', out.width='90%'}
knitr::include_graphics("images/dt-02.png")
```

---

# Growing the tree

.pull-left[

### Algorithms

- ID3 (Iterative Dichotomiser 3)
- C4.5 (successor of ID3)
- CART (Classification And Regression Tree)
- CHAID (CHi-squared Automatic Interaction Detector)
- MARS: (Multivariate Adaptive Regression Splines)
- Conditional Inference Trees
- and more...

]

---

# Growing the tree

.pull-left[

### Algorithms

- ID3 (Iterative Dichotomiser 3)
- C4.5 (successor of ID3)
- .bold.blue[CART (Classification And Regression Tree)]
- CHAID (CHi-squared Automatic Interaction Detector)
- MARS: (Multivariate Adaptive Regression Splines)
- Conditional Inference Trees
- and more...

]

.pull-right[

### CART Features `r anicon::faa("shopping-cart", animate = 'passing', speed = 'slow')`

- Classification and regression trees
- Continuous and discrete features
- Partitioning
   - Greedy top-down
   - Strictly binary splits (tends to produce tall/deep trees)
   - Variance reduction in regression trees
   - Gini impurity in classification trees
- Cost complexity pruning 
- [`r anicon::aia("google-scholar", animate = 'tada', anitype="hover", rtext = "(Breiman, 1984)")`](https://www.taylorfrancis.com/books/9781351460491)

]

<br>
.center[.content-box-gray[.bold[Most common decision tree algorithm]]]

---
# Best .red[`r anicon::nia("Binary", animate = "pulse")`] Partitioning

.pull-left[

.center.font130.bold[Regression tree]

```{r, echo=FALSE, out.width="90%"}
knitr::include_graphics("images/regression-partition.png")
```

]

.pull-right[

.center.font130.bold[Classification tree]

```{r, echo=FALSE, out.width="90%"}
knitr::include_graphics("images/classification-partition.png")
```

]

<br>
.center[.content-box-gray[.bold[Objective: Minimize disimilarity in terminal nodes]]]

---

# Best .red[Binary] Partitioning


.pull-left[
<br>
- __Numeric feature__: Numeric split to minimize loss function
<br><br><br><br><br>
- __Binary feature__: Category split to minimize loss function
<br><br><br><br><br>
- __Multiclass feature__: Order feature classes based on mean target variable (regression) or class proportion (classification) and choose split to minimize loss function ([`r anicon::aia("google-scholar", animate = 'tada', anitype="hover", rtext = "See ESL, section 9.2.4 for details")`](https://web.stanford.edu/~hastie/ElemStatLearn/)).

]

.pull-right[

```{r, echo=FALSE, out.height="55%", out.width="55%"}
knitr::include_graphics("images/splitting-rules.png")
```

]

---

# How deep to grow a tree?

Say we have the given data generated from the underlying .blue["truth"] function

<br><br>

```{r, echo=FALSE, fig.width=10, fig.height=5}
set.seed(1112)  # for reproducibility
df <- tibble::tibble(
  x = seq(from = 0, to = 2 * pi, length = 500),
  y = sin(x) + rnorm(length(x), sd = 0.5),
  truth = sin(x)
)
library(rpart)
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 1)
fit <- rpart(y ~ x, data = df, control = ctrl)
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1)
```

---

# Depth = 1 (decision .red[stump] `r anicon::cia("images/stump.png", animate = "pulse", anitype="hover")`)

.scrollable90[
.pull-left[

```{r, echo=FALSE, fig.width=6, fig.asp=0.3, out.width="100%"}
# fit single tree
library(rpart.plot)
par(mar=c(.5,.5,.5,.5))
rpart.plot(fit)
library(partykit)
as.party(fit)
```


]

.pull-right[

```{r, echo=FALSE, fig.height=6.5}
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred), color = "red", size = 1) +
  geom_segment(x = 3.1, xend = 3.1, y = -Inf, yend = -.75, lty = "dashed",
               arrow = arrow(length = unit(0.5,"cm"))) +
  annotate("text", x = 3.1, y = -Inf, label = "split", hjust = 1.2, vjust = -1)
```

]
]

---

# Depth = 3 `r anicon::cia("images/small-tree-icon.png", animate = "pulse", anitype="hover")`

.scrollable90[
.pull-left[

```{r, echo=FALSE, fig.width=6, fig.asp=0.6, out.width="100%"}
# fit single tree
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 3)
fit <- rpart(y ~ x, data = df, control = ctrl)
rpart.plot(fit)
as.party(fit)
```


]

.pull-right[

```{r, echo=FALSE, fig.height=6.5}
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred), color = "red", size = 1)
```

]
]

---

# Depth = 20 (.red[complex tree]  `r anicon::cia("images/large-tree-icon.png", animate = "pulse", anitype="hover")`)

.scrollable90[
.pull-left[

```{r, echo=FALSE, fig.height=9, fig.asp=1, out.height="100%"}
# fit single tree
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 20)
fit <- rpart(y ~ x, data = df, control = ctrl)
rpart.plot(fit)
```


]

.pull-right[

```{r, echo=FALSE, fig.height=6.5}
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred), color = "red", size = 1)
```

]
]

---

# .red[Two Predictor] Decision Boundaries

.pull-left[

### Classification problem: Iris data

```{r, echo = FALSE, fig.height=5}
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) +
  geom_point()
```

]



---

# .red[Two Predictor] Decision Boundaries

.pull-left[

### Classification problem: Iris data

```{r, echo = FALSE, fig.height=5}
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species, shape = Species)) +
  geom_point() +
  annotate("rect", xmin = -Inf, xmax = 5.44, ymin = 2.8, ymax = Inf, alpha = .75, fill = "orange") +
  annotate("text", x = 4.0, y = 4.4, label = "setosa", hjust = 0) +
  annotate("rect", xmin = -Inf, xmax = 5.44, ymin = 2.79, ymax = -Inf, alpha = .75, fill = "grey") +
  annotate("text", x = 4.0, y = 2, label = "versicolor", hjust = 0) +
  annotate("rect", xmin = 5.45, xmax = 6.15, ymin = 3.1, ymax = Inf, alpha = .75, fill = "orange") +
  annotate("text", x = 6, y = 4.4, label = "setosa", hjust = 1, vjust = 0) +
  annotate("rect", xmin = 5.45, xmax = 6.15, ymin = 3.09, ymax = -Inf, alpha = .75, fill = "grey") +
  annotate("text", x = 6, y = 2, label = "versicolor", hjust = 1, vjust = 0, fill = "grey") +
  annotate("rect", xmin = 6.16, xmax = Inf, ymin = -Inf, ymax = Inf, alpha = .75, fill = "green") +
  annotate("text", x = 8, y = 2, label = "virginica", hjust = 1, vjust = 0, fill = "green")
```

]

.pull-right[

### Classification tree

```{r, fig.height=5, fig.asp=.7, out.height="100%", echo=FALSE}
iris_fit <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)
rpart.plot(iris_fit)
```

]

---

# Minimize overfitting

.pull-left[

.font110[Must balance the depth and complexity of the tree to .bold[generalize] to unseen data]

2 main options:

* Early stopping 
   * Restrict tree depth
   * Restrict node size

* Pruning

]

.pull-right[

```{r, echo=FALSE, fig.height=6}
ctrl <- list(cp = 0, minbucket = 1, maxdepth = 50)
fit <- rpart(y ~ x, data = df, control = ctrl)
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred), color = "red", size = 1)
```

.center[.content-box-gray[.bold[Trees have a tendency to overfit]]]

]

---

# Minimize overfitting: Early stopping `r anicon::faa("stop-circle", animate = 'tada', anitype="hover", colour = "red")`

.pull-left[

.blue[Limit tree depth]: Stop splitting after a certain depth

```{r maxdepth, echo=FALSE, fig.height=5, message=FALSE, warning=FALSE}
maxdepth <- 1:15
results <- data.frame(NULL)
for(i in maxdepth) {
 ctrl <- list(cp = 0, maxdepth = i)
 fit <- rpart(y ~ x, data = df, control = ctrl) 
 
 predictions <- mutate(df, maxdepth = maxdepth[i])
 predictions$pred <- predict(fit, df)
 results <- rbind(results, predictions)
   
}
p <- results %>%
  mutate(
    truth = sin(x),
    
    ) %>%
  ggplot(aes(x, pred)) +
  geom_point(aes(x, y), alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(color = "red", size = 1) +
  labs(title = 'Max depth of tree: {frame_time}') +
  transition_time(-maxdepth)
animate(p, renderer = gifski_renderer(), device = "png")
```

]


--

.pull-right[

.blue[Minimum node “size”]: Do not split intermediate node which contains too few data points

```{r minbucket, echo=FALSE, fig.height=5}
minbucket <- 1:15
results <- data.frame(NULL)
for(i in minbucket) {
 ctrl <- list(cp = 0, minbucket = i)
 fit <- rpart(y ~ x, data = df, control = ctrl) 
 
 predictions <- mutate(df, minbucket = minbucket[i])
 predictions$pred <- predict(fit, df)
 results <- rbind(results, predictions)
   
}
p <- ggplot(results, aes(x, pred)) +
  geom_point(data = results, aes(x, y), alpha = .3, size = 2) +
  geom_line(data = results, aes(x, y = truth), color = "blue", size = 1) +
  geom_line(color = "red", size = 1) +
  labs(title = 'Minimum number of obs in each terminal node: {frame_time}') +
  transition_time(minbucket)
animate(p, renderer = gifski_renderer(), device = "png")
```


]

---
class: clear, center, middle

background-image: url(images/any-questions.jpg)
background-position: center
background-size: cover

---
# Back home

<br><br><br><br>
[.center[`r anicon::faa("home", size = 10, animate = FALSE)`]](https://github.com/misk-data-science/misk-homl)

.center[https://github.com/misk-data-science/misk-homl]
