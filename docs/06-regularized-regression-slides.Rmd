---
title: "Regularized Regression"
author: "Misk Academy"
date: "2020-6-22"
output:
  xaringan::moon_reader:
    css: ["custom.css"]
    self_contained: false
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false 
---

```{r setup, include=FALSE, cache=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center", 
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)
library(tidyverse)
# set ggplot to black and white theme
library(ggplot2)
theme_set(theme_bw())
```

class: center, middle, inverse

.font300.white[Regularized Regression]

---
# Prerequisites

.pull-left[

.center.bold.font120[Packages]

```{r}
# Helper packages
library(recipes)   # for feature engineering
library(tidyverse) # general data munging

# Modeling packages
library(glmnet)   # for implementing regularized regression
library(caret)    # for automating the tuning process
library(rsample)  # for sampling

# Model interpretability packages
library(vip)      # for variable importance
```

]

.pull-right[

.center.bold.font120[Data]

```{r prereqs-data}
# ames data
ames <- AmesHousing::make_ames()
# split data
set.seed(123)
split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(split)
```

]

---
# The Idea

.font120[As *p* grows larger, there are three main issues we most commonly run into:]

1. Multicollinearity (we've already seen how PCR & PLS help to resolve this)

2. Insufficient solution ( $p >> n$ )

3. Interpretability
   - Approach 1: model selection
      - computationally inefficient (Ames data: $2^{80}$ models to evaluate)
      - simply assume a feature as in or out $\rightarrow$ _hard threshholding_
   - Approach 2: regularize
      - retain all coefficients
      - slowly pushes a feature's effect towards zero $\rightarrow$ _soft threshholding_
   
--

<br>
.center.bold.blue[Regularization helps with all three of these issues!]

---
# Regular regression

<br>

\begin{equation}
\text{minimize} \bigg \{ SSE = \sum^n_{i=1} (y_i - \hat{y}_i)^2 \bigg \}
\end{equation}

```{r, echo=FALSE, fig.height=5, fig.width=10}
ames_sub <- ames_train %>%
  filter(Gr_Liv_Area > 1000 & Gr_Liv_Area < 3000) %>%
  sample_frac(.5)
model1 <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_sub)
model1 %>%
  broom::augment() %>%
  ggplot(aes(Gr_Liv_Area, Sale_Price)) + 
  geom_segment(aes(x = Gr_Liv_Area, y = Sale_Price,
                   xend = Gr_Liv_Area, yend = .fitted), 
               alpha = 0.3) +
  geom_point(size = 1, color = "red") +
  geom_smooth(se = FALSE, method = "lm") +
  scale_y_continuous(labels = scales::dollar)
```

---
# Regular.red[ized] regression

<br>

\begin{equation}
\text{minimize} \big \{ SSE + P \big \}
\end{equation}

<br>

Modify OLS objective function by adding a ___.red[P]enalty___ parameter 

- Constrains magnitude of the coefficients

- Progressively shrinks coefficients to zero

- Reduces variability of coefficients (pulls correlated coefficients together)

- Can automate feature selection


.center.bold.blue[There are 3 variants of regularized regression]

---
# .red[Ridge] regression

.pull-left[
Objective function: 

\begin{equation}
\text{minimize } \bigg \{ SSE + \lambda \sum^p_{j=1} \beta_j^2 \bigg \}
\end{equation}

* referred to as $L_2$ penalty

* pulls correlated features towards each other

* pushes coefficients to .red[near zero]

* retains .red[all] features

]

.pull-right[

```{r ridge-coef-example, echo=FALSE, fig.height=5}
boston_train_x <- model.matrix(cmedv ~ ., pdp::boston)[, -1]
boston_train_y <- pdp::boston$cmedv
# model
boston_ridge <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = 0
)
lam <- boston_ridge$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_ridge$a0 %>% names()) %>%
  rename(lambda = ".")
results <- boston_ridge$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)
result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5))
ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), nudge_x = -.06, show.legend = FALSE)
```

```{r lambda, echo=FALSE}
knitr::include_graphics("images/lambda.001.png")
```

]

---
# .red[Lasso] regression

.pull-left[
Objective function: 

\begin{equation}
\text{minimize } \bigg \{ SSE + \lambda \sum^p_{j=1} | \beta_j | \bigg \}
\end{equation}

* referred to as $L_1$ penalty

* pulls correlated features towards each other

* pushes coefficients to .red[zero]

* performs .red[automated feature selection]

]

.pull-right[

```{r lasso-coef-example, echo=FALSE, fig.height=5}
# model
boston_lasso <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = 1
)
lam <- boston_lasso$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_lasso$a0 %>% names()) %>%
  rename(lambda = ".")
results <- boston_lasso$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)
result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5))
ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), nudge_x = -.05, show.legend = FALSE)
```

```{r lambda2, echo=FALSE}
knitr::include_graphics("images/lambda.001.png")
```

]

---
# .red[Elastic net] regression

.pull-left[
Objective function: 

\begin{equation}
\text{minimize } \bigg \{ SSE + \lambda_1 \sum^p_{j=1} \beta_j^2 + \lambda_2 \sum^p_{j=1} | \beta_j | \bigg \}
\end{equation}

* combines $L_1$ & $L_2$ penalties

* provides best of both worlds

]

.pull-right[

```{r elastic-net-coef-example, echo=FALSE, fig.height=5}
# model
boston_elastic <- glmnet::glmnet(
  x = boston_train_x,
  y = boston_train_y,
  alpha = .2
)
lam <- boston_elastic$lambda %>% 
  as.data.frame() %>%
  mutate(penalty = boston_elastic$a0 %>% names()) %>%
  rename(lambda = ".")
results <- boston_elastic$beta %>% 
  as.matrix() %>% 
  as.data.frame() %>%
  rownames_to_column() %>%
  gather(penalty, coefficients, -rowname) %>%
  left_join(lam)
result_labels <- results %>%
  group_by(rowname) %>%
  filter(lambda == min(lambda)) %>%
  ungroup() %>%
  top_n(5, wt = abs(coefficients)) %>%
  mutate(var = paste0("x", 1:5))
ggplot() +
  geom_line(data = results, aes(lambda, coefficients, group = rowname, color = rowname), show.legend = FALSE) +
  scale_x_log10() +
  geom_text(data = result_labels, aes(lambda, coefficients, label = var, color = rowname), nudge_x = -.05, show.legend = FALSE)
```

```{r lambda3, echo=FALSE}
knitr::include_graphics("images/lambda.001.png")
```

]

---
# Tuning

.pull-left[

* .bold[lambda]
   - controls the magnitude of the penalty parameter
   - rule of `r anicon::cia("https://emojis.slackmojis.com/emojis/images/1511903783/3230/wiggle_thumbs_up.gif?1511903783", animate = "slow")`: 0.1, 10, 100, 1000, 10000

* .bold[alpha]
   - controls the type of penalty (ridge, lasso, elastic net)
   - rule of `r anicon::cia("https://emojis.slackmojis.com/emojis/images/1511903783/3230/wiggle_thumbs_up.gif?1511903783", animate = "slow")`: 0, .25, .50, .75, 1

]

.pull-right[

<br>
.center[.bold[Tip]: find tuning parameters with:]

```{r show-tuning-parameters}
caret::getModelInfo("glmnet")$glmnet$parameters
```

.center[Here, "glmnet" represents the __caret__ method we are going to use]

]

---
# R packages `r emo::ji("package")`

.pull-left[

## [`glmnet`](https://cran.r-project.org/package=glmnet)

* original implementation of regularized regression in R

* linear regression, logistic and multinomial regression models, Poisson regression and the Cox model

* extremely efficient procedures for fitting the entire lasso or elastic-net regularization path

]
.pull-right[

## [h2o](https://cran.r-project.org/package=h2o) `r emo::ji("droplet")`

* java-based interface

* Automated feature pre-processing & validation procedures

* Supports the following distributions: “guassian”, “binomial”, “multinomial”, “ordinal”, “poisson”, “gamma”, “tweedie”
    
]

.center.bold[Other options exist (see __Regularized and Shrinkage Methods__ section of [Machine Learning task view](https://CRAN.R-project.org/view=MachineLearning
)) but these are the preferred.]

---
# Data prep

.pull-left[

* glmnet only accepts the non-formula XY interface so prior to modeling we need to separate our feature and target sets and

* dummy encode our feature set 

]

.pull-right[
```{r}
# Create training  feature matrices
# we use model.matrix(...)[, -1] to discard the intercept
X <- model.matrix(Sale_Price ~ ., ames_train)[, -1]

# transform y with log transformation
Y <- log(ames_train$Sale_Price)
```
]
---
# glmnet

.bold[Pro Tip]: glmnet can auto-generate the appropriate λ values based on the data; the vast majority of the time you will have little need to adjust this default.

.scrollable90[
.pull-left[

.center.bold[Ridge]

```{r}
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

plot(ridge, xvar = "lambda")
```


]

.pull-right[

.center.bold[Lasso]

```{r}
lasso <- glmnet(
  x = X,
  y = Y,
  alpha = 1
)

plot(lasso, xvar = "lambda")
```

]
]


---
# glmnet

* So which one is better?

--

* We can use `cv.glmnet` to provide cross-validated results

.scrollable90[
.pull-left[

.center.bold[Ridge]

```{r}
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

plot(ridge)
```


]

.pull-right[

.center.bold[Lasso]

```{r}
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)

plot(lasso)
```

]
]

---
# glmnet

* So which one is better?
* We can use `cv.glmnet` to provide cross-validated results
* The results are similar but the lasso model provides feature selection --> allows us to focus on only 64 features rather than 296!

.code70.scrollable90[
.pull-left[

.center.bold[Ridge]

```{r}
# Ridge model - minimum MSE
min(ridge$cvm)

# Ridge model - lambda for this min MSE
ridge$lambda.min 

# Ridge model w/1-SE rule
ridge$cvm[ridge$lambda == ridge$lambda.1se]

# Ridge model w/1-SE rule -- No. of coef | 1-SE MSE
ridge$nzero[ridge$lambda == ridge$lambda.1se]
```


]

.pull-right[

.center.bold[Lasso]

```{r}
# Lasso model - minimum MSE
min(lasso$cvm)       

# Lasso model - lambda for this min MSE
lasso$lambda.min 

# Lasso model - w/1-SE rule
lasso$cvm[lasso$lambda == lasso$lambda.1se]

# Lasso model w/1-SE rule -- No. of coef | 1-SE MSE
lasso$nzero[lasso$lambda == lasso$lambda.1se]
```

]
]

---
# Grid search

Often, the optimal model contains an alpha somewhere between 0–1, thus we want to tune both the λ and the alpha parameters. 

.scrollable90[
.pull-left[
```{r cv-glmnet}
# tuning grid
hyper_grid <- expand.grid(
  alpha = seq(0, 1, by = .25),
  lambda = c(0.1, 10, 100, 1000, 10000)
)

# perform resampling
set.seed(123)
cv_glmnet <- train(
  x = X,
  y = Y,
  method = "glmnet",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# best model
cv_glmnet$results %>%
  filter(
    alpha == cv_glmnet$bestTune$alpha,
    lambda == cv_glmnet$bestTune$lambda
    )
```
]

.pull-right[

```{r cv-glmnet-plot, fig.height=5}
# plot results
plot(cv_glmnet)
```

]
]

---
# Comparing results to previous models

.pull-left[

* So how does this compare to our previous best model for the Ames data set? 

* Keep in mind that for this module we log transformed the response variable (`Sale_Price`). 

* Consequently, to provide a fair comparison to our previously model(s) we need to re-transform our predicted values.

]

.pull-right[

```{r}
# predict sales price on training data
pred <- predict(cv_glmnet, X)

# compute RMSE of transformed predicted
RMSE(exp(pred), exp(Y))
```

]

---
# Feature interpretation

```{r}
vip(cv_glmnet, num_features = 20, geom = "point")
```

---
# Feature interpretation

```{r regularized-top4-pdp, echo=FALSE, fig.height=8, fig.width=12}
p1 <- pdp::partial(cv_glmnet, pred.var = "Gr_Liv_Area", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(Gr_Liv_Area, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)

p2 <- pdp::partial(cv_glmnet, pred.var = "Total_Bsmt_SF", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(Total_Bsmt_SF, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)

p3 <- pdp::partial(cv_glmnet, pred.var = "Overall_QualExcellent") %>%
  mutate(
    yhat = exp(yhat),
    Overall_QualExcellent = factor(Overall_QualExcellent)
    ) %>%
  ggplot(aes(Overall_QualExcellent, yhat)) +
  geom_boxplot() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)

p4 <- pdp::partial(cv_glmnet, pred.var = "Year_Built", grid.resolution = 20) %>%
  mutate(yhat = exp(yhat)) %>%
  ggplot(aes(Year_Built, yhat)) +
  geom_line() +
  scale_y_continuous(limits = c(0, 300000), labels = scales::dollar)

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

---
class: clear, center, middle

background-image: url(images/any-questions.jpg)
background-position: center
background-size: cover

---
# Back home

<br><br><br><br>
[.center[`r anicon::faa("home", size = 10, animate = FALSE)`]](https://github.com/misk-data-science/misk-homl)

.center[https://github.com/misk-data-science/misk-homl]
