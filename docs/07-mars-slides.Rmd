---
title: "Multivariate Adaptive Regression Splines"
author: "Misk Academy"
date: "2020-6-22"
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    css: ["custom.css"]
    self_contained: false
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false 
---

```{r setup, include=FALSE, cache=FALSE}
# Set global R options
options(htmltools.dir.version = FALSE, servr.daemon = TRUE)
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center", 
  cache = TRUE,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE 
)
library(tidyverse)
# set ggplot to black and white theme
library(ggplot2)
theme_set(theme_bw())
```

class: misk-title-slide   

<br><br><br><br><br>
# .font140[Multivariate Adaptive Regression Splines]

---
# Prerequisites

.pull-left[

.center.bold.font120[Packages]

```{r 10-pkgs, message=FALSE}
# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting

# Modeling packages
library(earth)     # for fitting MARS models
library(caret)     # for automating the tuning process

# Model interpretability packages
library(vip)       # for variable importance
library(pdp)       # for variable relationships
```

]

.pull-right[

.center.bold.font120[Data]

```{r prereqs-data}
# ames data
ames <- AmesHousing::make_ames()

# split data
set.seed(123)
split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(split)
```

]

---
# The idea

* So far, we have tried to improve our linear model with various feature reduction and regularization approaches

* However, we are still assuming linear relationships

* The actual relationship(s) may have non-linear patterns that we cannot capture

```{r non-linearity, fig.height=5, fig.width=9, echo=FALSE}
# Simulate some nonlinear monotonic data
set.seed(123)  # for reproducibility
x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = 0.3)
df <- data.frame(x, y) %>%
  filter(x < 6)
ggplot(df, aes(x, y)) +
  geom_point(size = 1, alpha = .5) +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Assumed linear relationship")
```

---
# The idea

.font120[
* There are some traditional approaches we could take to capture non-linear relationships:
   - polynomial relationships
   - step function relationships
]

```{r traditional-nonlinear-approaches, fig.height=3.5, fig.width=12, echo=FALSE}
p1 <- ggplot(df, aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  stat_smooth( method = "lm", se = FALSE, formula = y ~ poly(x, 2, raw = TRUE)) +
  ggtitle("(A) Degree-2 polynomial regression")
p2 <- ggplot(df, aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  stat_smooth( method = "lm", se = FALSE, formula = y ~ poly(x, 3, raw = TRUE)) +
  ggtitle("(B) Degree-3 polynomial regression")
# fit step function model (5 steps)
step_fit <- lm(y ~ cut(x, 5), data = df)
step_pred <- predict(step_fit, df)
p3 <- ggplot(cbind(df, step_pred), aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = step_pred), size = 1, color = "blue") +
  ggtitle("(C) Step function regression")
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

<br>

.center.bold.blue[However, these require the user explicitly identify & incorporate `r anicon::cia("https://emojis.slackmojis.com/emojis/images/1542340473/4983/yuck.gif?1542340473", animate = "slow", size = 2)`]

---
# The idea

.pull-left[

* Multivariate adaptive regression splines (MARS) provide a convenient & automated approach to capture non-linearity

* Easy transition from linear regression to non-linearity methods

* Looks for .blue[knots] in predictors

<br><br>

\begin{equation}
  \text{y} = 
  \begin{cases}
    \beta_0 + \beta_1(1.183606 - \text{x}) & \text{x} < 1.183606, \\
    \beta_0 + \beta_1(\text{x} - 1.183606) & \text{x} > 1.183606
  \end{cases}
\end{equation}

]

.pull-right[

```{r one-knot, echo=FALSE, fig.height=5}
mars1 <- mda::mars(
  df$x,
  df$y,
  nk = 3,
  prune = FALSE
  )
df %>%
  mutate(predicted = as.vector(mars1$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("One knot")
```

]

---
# The idea

.pull-left[

* Multivariate adaptive regression splines (MARS) provide a convenient & automated approach to capture non-linearity

* Easy transition from linear regression to non-linearity methods

* Looks for .blue[knots] in predictors

<br><br>

\begin{equation}
  \text{y} = 
  \begin{cases}
    \beta_0 + \beta_1(1.183606 - \text{x}) & \text{x} < 1.183606, \\
    \beta_0 + \beta_1(\text{x} - 1.183606) & \text{x} > 1.183606 \quad \& \quad \text{x} < 4.898114, \\
    \beta_0 + \beta_1(4.898114 - \text{x}) & \text{x} > 4.898114
  \end{cases}
\end{equation}

]

.pull-right[

```{r two-knots, echo=FALSE, fig.height=5}
mars2 <- mda::mars(
  df$x,
  df$y,
  nk = 5,
  prune = FALSE
  )
df %>%
  mutate(predicted = as.vector(mars2$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("Two knots")
```

]

---
# The idea

.pull-left[

* Multivariate adaptive regression splines (MARS) provide a convenient & automated approach to capture non-linearity

* Easy transition from linear regression to non-linearity methods

* Looks for .blue[knots] in predictors

]

.pull-right[

```{r three-knots, echo=FALSE, fig.height=5}
mars3 <- mda::mars(
  df$x,
  df$y,
  nk = 7,
  prune = FALSE
  )
df %>%
  mutate(predicted = as.vector(mars3$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("Three knots")
```

]

---
# The idea

.pull-left[

* Multivariate adaptive regression splines (MARS) provide a convenient & automated approach to capture non-linearity

* Easy transition from linear regression to non-linearity methods

* Looks for .blue[knots] in predictors

]

.pull-right[

```{r four-knots, echo=FALSE, fig.height=5}
mars3 <- mda::mars(
  df$x,
  df$y,
  nk = 9,
  prune = FALSE
  )
df %>%
  mutate(predicted = as.vector(mars3$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("Four knots")
```

]

---
# The idea

.pull-left[

* Multivariate adaptive regression splines (MARS) provide a convenient & automated approach to capture non-linearity

* Easy transition from linear regression to non-linearity methods

* Looks for .blue[knots] in predictors

]

.pull-right[

```{r nine-knots, echo=FALSE, fig.height=5}
mars3 <- mda::mars(
  df$x,
  df$y,
  nk = 20,
  prune = FALSE
  )
df %>%
  mutate(predicted = as.vector(mars3$fitted.values)) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 1, alpha = .2) +
  geom_line(aes(y = predicted), size = 1, color = "blue") +
  ggtitle("Five knots")
```

]

---
# R packages `r emo::ji("package")`

.pull-left[
## [`mda`](https://cran.r-project.org/package=mda)
* **m**ixture **d**iscriminant **a**nalysis
* Lightweight function `mars()`
    
* Gives quite similar results to Friedman's original FORTRAN program
* No formula method
]
.pull-right[
## [`earth`](http://www.milbo.users.sonic.net/earth/) `r emo::ji("earth_americas")`
* **e**nhanced **a**daptive **r**egression **t**hrough **h**inges
* Derived from `mda::mars()`
    
* Support for GLMs (e.g., logistic regression)
    
* More bells and whistles than `mda::mars()`; for example,
    - Variable importance scores
    
    - Support for $k$-fold cross-validation)
    
]

---
# Tuning parameters

MARS models have two tuning parameters:

.pull-left[

1. .blue[_nprune_]: the maximum number of terms in the pruned model (including the intercept)

2. .blue[_degree_]: the maximum degree of interaction

]

.pull-right[

```{r earth-tuning-params}
caret::getModelInfo("earth")$earth$parameters
```

]

---
# Implementation

.scrollable90[
.pull-left[
```{r cv-mars}
# tuning grid
hyper_grid <- expand.grid(
  nprune = seq(2, 50, length.out = 10) %>% floor(),
  degree = 1:3
)

# perform resampling
set.seed(123)
cv_mars <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  trControl = trainControl(method = "cv", number = 10),
  method = "earth", #<<
  tuneGrid = hyper_grid,
  metric = "RMSE"
  )

# best model
cv_mars$results %>%
  filter(
    nprune == cv_mars$bestTune$nprune,
    degree == cv_mars$bestTune$degree
    )
```
]

.pull-right[

```{r cv-mars-plot, fig.height=5}
# plot results
plot(cv_mars)
```

]
]

---
# Feature importance

* Backwards elimination feature selection routine that looks at reductions in the GCV estimate of error as each predictor is added to the model. 
* This total reduction is used as the variable importance measure (`value = "gcv"`). 
* Can also monitor the change in the residual sums of squares (RSS) as terms are added (`value = "rss"`)

.bold.center[Automated feature selection]

.scrollable90[

```{r mars-vip, fig.height=10, fig.width=15}
p1 <- vip(cv_mars, num_features = 40, geom = "point", value = "gcv") + ggtitle("GCV")
p2 <- vip(cv_mars, num_features = 40, geom = "point", value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

]

---
# Partial dependence plots

```{r pdp, fig.width=15, fig.height=3, warning=FALSE, message=FALSE}
# Construct partial dependence plots
p1 <- partial(cv_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  ggplot(aes(Gr_Liv_Area, yhat)) +
  geom_line()
p2 <- partial(cv_mars, pred.var = "Year_Built", grid.resolution = 10) %>% 
  ggplot(aes(Year_Built, yhat)) +
  geom_line()
p3 <- partial(cv_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), 
              grid.resolution = 10) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, 
              screen = list(z = -20, x = -60))
# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

---
class: clear, center, middle, hide-logo

background-image: url(images/any-questions.jpg)
background-position: center
background-size: cover

---
# Back home

<br><br><br><br>
[.center[`r anicon::faa("home", size = 10, animate = FALSE)`]](https://github.com/misk-data-science/misk-homl)

.center[https://github.com/misk-data-science/misk-homl]
