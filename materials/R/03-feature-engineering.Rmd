---
title: "Chapter 3: Feature & Target Engineering"
output: html_notebook
---

__Note__: Some results may differ from the hard copy book due to the changing of sampling procedures introduced in R 3.6.0. See http://bit.ly/35D1SW7 for more details. Access and run the source code for this notebook [here](https://rstudio.cloud/project/801185). 

## Prerequisites

This chapter leverages the following packages:

```{r engineering-prereqs}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(visdat)   # for additional visualizations

# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
```

We'll also continue working with the `ames_train` data set:

```{r engineering-ames-train, echo=TRUE}
ames <- AmesHousing::make_ames()

# Load and split the Ames housing data using stratified sampling
set.seed(123)  # for reproducibility
split  <- rsample::initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  <- rsample::training(split)
ames_test   <- rsample::testing(split)
```

## Target engineering

```{r engineering-example-log, eval=FALSE}
transformed_response <- log(ames_train$Sale_Price)
```

```{r engineering-y_log}
# log transformation
ames_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_log(all_outcomes())
ames_recipe
```

```{r engineering-neg-log, error=TRUE}
log(-0.5)
log1p(-0.5)
```




```{r engineering-eninverse-bc}
# Log transform a value
y <- log(10)

# Undo log-transformation
exp(y)

# Box Cox transform a value
y <- forecast::BoxCox(10, lambda)

# Inverse Box Cox function
inv_box_cox <- function(x, lambda) {
  # for Box-Cox, lambda = 0 --> log transform
  if (lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda) 
}

# Undo Box Cox-transformation
inv_box_cox(y, lambda)
```

## Dealing with missingness

```{r engineering-ames-raw-missing}
sum(is.na(AmesHousing::ames_raw))
```

Figure 3.3:

```{r engineering-heat-map-missingness, fig.width=8, fig.height=5, out.width="100%", fig.cap = "Heat map of missing values in the raw Ames housing data."}
AmesHousing::ames_raw %>%
  is.na() %>%
  reshape2::melt() %>%
  ggplot(aes(Var2, Var1, fill=value)) + 
    geom_raster() + 
    coord_flip() +
    scale_y_continuous(NULL, expand = c(0, 0)) +
    scale_fill_grey(name = "", 
                    labels = c("Present", 
                               "Missing")) +
    xlab("Observation") +
    theme(axis.text.y  = element_text(size = 4))
```

```{r engineering-missingness-garages}
AmesHousing::ames_raw %>% 
  filter(is.na(`Garage Type`)) %>% 
  select(`Garage Type`, `Garage Cars`, `Garage Area`)
```

Figure 3.4:

```{r engineering-missingness-visna, fig.height=7, fig.width=12, fig.cap="Visualizing missing data patterns in the raw Ames housing data."}
vis_miss(AmesHousing::ames_raw, cluster = TRUE)
```

### Imputation

```{r engineering-mean-impute}
ames_recipe %>%
  step_medianimpute(Gr_Liv_Area)
```

```{r engineering-knn-impute}
ames_recipe %>%
  step_knnimpute(all_predictors(), neighbors = 6)
```

```{r engineering-bagging-impute}
ames_recipe %>%
  step_bagimpute(all_predictors())
```

Figure 3.5:

```{r engineering-imputation-examples, echo=TRUE, fig.cap="Comparison of three different imputation methods. The red points represent actual values which were removed and made missing and the blue points represent the imputed values. Estimated statistic imputation methods (i.e. mean, median) merely predict the same value for each observation and can reduce the signal between a feature and the response; whereas KNN and tree-based procedures tend to maintain the feature distribution and relationship."}
impute_ames <- ames_train
set.seed(123)
index <- sample(seq_along(impute_ames$Gr_Liv_Area), 50)
actuals <- ames_train[index, ]
impute_ames$Gr_Liv_Area[index] <- NA

p1 <- ggplot() +
  geom_point(data = impute_ames, aes(Gr_Liv_Area, Sale_Price), alpha = .2) +
  geom_point(data = actuals, aes(Gr_Liv_Area, Sale_Price), color = "red") +
  scale_x_log10(limits = c(300, 5000)) +
  scale_y_log10(limits = c(10000, 500000)) +
  ggtitle("Actual values")

# Mean imputation
mean_juiced <- recipe(Sale_Price ~ ., data = impute_ames) %>%
  step_meanimpute(Gr_Liv_Area) %>%
  prep(training = impute_ames, retain = TRUE) %>%
  juice()
mean_impute <- mean_juiced[index, ]
  
p2 <- ggplot() +
  geom_point(data = actuals, aes(Gr_Liv_Area, Sale_Price), color = "red") +
  geom_point(data = mean_impute, aes(Gr_Liv_Area, Sale_Price), color = "blue") +
  scale_x_log10(limits = c(300, 5000)) +
  scale_y_log10(limits = c(10000, 500000)) +
  ggtitle("Mean Imputation")

# KNN imputation
knn_juiced <- recipe(Sale_Price ~ ., data = impute_ames) %>%
  step_knnimpute(Gr_Liv_Area) %>%
  prep(training = impute_ames, retain = TRUE) %>%
  juice()
knn_impute <- knn_juiced[index, ]
  
p3 <- ggplot() +
  geom_point(data = actuals, aes(Gr_Liv_Area, Sale_Price), color = "red") +
  geom_point(data = knn_impute, aes(Gr_Liv_Area, Sale_Price), color = "blue") +
  scale_x_log10(limits = c(300, 5000)) +
  scale_y_log10(limits = c(10000, 500000)) +
  ggtitle("KNN Imputation")

# Bagged imputation
bagged_juiced <- recipe(Sale_Price ~ ., data = impute_ames) %>%
  step_bagimpute(Gr_Liv_Area) %>%
  prep(training = impute_ames, retain = TRUE) %>%
  juice()
bagged_impute <- bagged_juiced[index, ]
  
p4 <- ggplot() +
  geom_point(data = actuals, aes(Gr_Liv_Area, Sale_Price), color = "red") +
  geom_point(data = bagged_impute, aes(Gr_Liv_Area, Sale_Price), color = "blue") +
  scale_x_log10(limits = c(300, 5000)) +
  scale_y_log10(limits = c(10000, 500000)) +
  ggtitle("Bagged Trees Imputation")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

## Feature filtering 

Figure 3.6 (generated from benchmark data performed in the past):

```{r engineering-accuracy-comparison, echo=TRUE, fig.width=10, fig.height=3.5, fig.cap="Test set RMSE profiles when non-informative predictors are added."}
model_results <- read_csv("data/feature-selection-impacts-results.csv") %>%
  mutate(type = case_when(
    model %in% c("lm", "pcr", "pls", "glmnet", "lasso") ~ "Linear models",
    model %in% c("earth", "svmLinear", "nn") ~ "Non-linear Models",
    TRUE ~ "Tree-based Models"
  )) %>%
  mutate(model = case_when(
    model == "lm" ~ "Linear regression",
    model == "earth" ~ "Multivariate adaptive regression splines",
    model == "gbm" ~ "Gradient boosting machines",
    model == "glmnet" ~ "Elastic net",
    model == "lasso" ~ "Lasso",
    model == "nn" ~ "Neural net",
    model == "pcr" ~ "Principal component regression",
    model == "pls" ~ "Partial least squares",
    model == "ranger" ~ "Random forest",
    TRUE ~ "Support vector machine"
  ))

ggplot(model_results, aes(NIP, RMSE, color = model, lty = model)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ type, nrow = 1) +
  xlab("Number of additional non-informative predictors")
```

Figure 3.7 (generated from benchmark data performed in the past):

```{r engineering-impact-on-time, echo=TRUE, fig.width=10, fig.height=3.5, fig.cap="Impact in model training time as non-informative predictors are added."}
model_results %>%
  group_by(model) %>%
  mutate(
    time_impact = time / first(time),
    time_impact = time_impact - 1
  ) %>%
  ggplot(aes(NIP, time_impact, color = model, lty = model)) +
    geom_line() +
    geom_point() +
    facet_wrap(~ type, nrow = 1) +
    scale_y_continuous("Percent increase in training time", 
                       labels = scales::percent) +
    xlab("Number of additional non-informative predictors")
```

```{r engineering-nzv}
caret::nearZeroVar(ames_train, saveMetrics = TRUE) %>% 
  tibble::rownames_to_column() %>% 
  filter(nzv)
```

## Numeric feature engineering

```{r engineering-normalizing}
# Normalize all numeric columns
recipe(Sale_Price ~ ., data = ames_train) %>%
  step_YeoJohnson(all_numeric())                 
```

Figure 3.8:

```{r engineering-standardizing, echo=TRUE, fig.height=3, fig.cap="Standardizing features allows all features to be compared on a common value scale regardless of their real value differences."}
set.seed(123)
x1 <- tibble(
  variable = "x1",
  `Real value` = runif(25, min = -30, max = 5),
  `Standardized value` = scale(`Real value`) %>% as.numeric()
)

set.seed(456)
x2 <- tibble(
  variable = "x2",
  `Real value` = rlnorm(25, log(25)),
  `Standardized value` = scale(`Real value`) %>% as.numeric()
)

set.seed(789)
x3 <- tibble(
  variable = "x3",
  `Real value` = rnorm(25, 150, 15),
  `Standardized value` = scale(`Real value`) %>% as.numeric()
)

x1 %>%
  bind_rows(x2) %>%
  bind_rows(x3) %>%
  gather(key, value, -variable) %>%
  mutate(variable = factor(variable, levels = c("x3", "x2", "x1"))) %>%
  ggplot(aes(value, variable)) +
    geom_point(alpha = .6) +
    facet_wrap(~ key, scales = "free_x") +
    ylab("Feature") +
    xlab("Value")
```

```{r engineering-standardizing-recipes}
ames_recipe %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())
```


## Categorical feature engineering

```{r engineering-overall-qual-levels}
count(ames_train, Neighborhood) %>% arrange(n)
```

```{r engineering-screen-porch}
count(ames_train, Screen_Porch) %>% arrange(n)
```

```{r engineering-tbd}
# Lump levels for two features
lumping <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_other(Neighborhood, threshold = 0.01, 
             other = "other") %>%
  step_other(Screen_Porch, threshold = 0.1, 
             other = ">0")

# Apply this blue print --> you will learn about this at 
# the end of the chapter
apply_2_training <- prep(lumping, training = ames_train) %>%
  bake(ames_train)

# New distribution of Neighborhood
count(apply_2_training, Neighborhood) %>% arrange(n)

# New distribution of Screen_Porch
count(apply_2_training, Screen_Porch) %>% arrange(n)
```

```{r engineering-one-hot, echo=TRUE, fig.cap='Eight observations containing a categorical feature X and the difference in how one-hot and dummy encoding transforms this feature.', out.height="99%", out.width="99%"}
knitr::include_graphics("images/ohe-vs-dummy.png")
```

```{r engineering-tbd2}
# Lump levels for two features
recipe(Sale_Price ~ ., data = ames_train) %>%
  step_dummy(all_nominal(), one_hot = TRUE)
```

```{r engineering-label-encoding}
# Original categories
count(ames_train, MS_SubClass)

# Label encoded
recipe(Sale_Price ~ ., data = ames_train) %>%
  step_integer(MS_SubClass) %>%
  prep(ames_train) %>%
  bake(ames_train) %>%
  count(MS_SubClass)
```

```{r engineering-qual-variables}
ames_train %>% select(contains("Qual"))
```

```{r engineering-ordinal-encoding}
# Original categories
count(ames_train, Overall_Qual)

# Label encoded
recipe(Sale_Price ~ ., data = ames_train) %>%
  step_integer(Overall_Qual) %>%
  prep(ames_train) %>%
  bake(ames_train) %>%
  count(Overall_Qual)
```

```{r engineering-target-encoding, echo=TRUE, fig.cap='Example of target encoding the Neighborhood feature of the Ames housing data set.'}
ames_train %>%
  group_by(Neighborhood) %>%
  summarize(`Avg Sale_Price` = mean(Sale_Price, na.rm = TRUE)) %>%
  head(10) %>%
  kable(caption = "Example of target encoding the Neighborhood feature of the Ames housing data set.") %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

```{r engineering-proportion-encoding, echo=TRUE, fig.cap='Example of categorical proportion encoding the Neighborhood feature of the Ames housing data set.'}
ames_train %>%
  count(Neighborhood) %>%
  mutate(Proportion = n / sum(n)) %>%
  select(-n) %>%
  head(10) %>%
  kable(caption = 'Example of categorical proportion encoding the Neighborhood feature of the Ames housing data set.') %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

## Dimension reduction

```{r engineering-pca}
recipe(Sale_Price ~ ., data = ames_train) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_pca(all_numeric(), threshold = .95)
```

## Proper implementation

Figure 3.10:

```{r engineering-minimize-leakage, echo=TRUE, fig.cap="Performing feature engineering preprocessing within each resample helps to minimize data leakage.", out.width='90%'}
knitr::include_graphics("images/minimize-leakage.png")
```

```{r engineering-step1-recipe}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal())  %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_pca(all_numeric(), -all_outcomes())
  
blueprint
```

```{r engineering-step2-prepare}
prepare <- prep(blueprint, training = ames_train)
prepare
```

```{r engineering-step3-bake}
baked_train <- bake(prepare, new_data = ames_train)
baked_test <- bake(prepare, new_data = ames_test)
baked_train
```

```{r engineering-knn-blueprint}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

```{r engineering-knn-with-blueprint}
# Specify resampling plan
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

# Construct grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit2 <- train(
  blueprint, 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)
```

```{r engineering-knn-with-blueprint-assess, fig.height=3, fig.cap="Results from the same grid search performed in Section 2.7 but with feature engineering performed within each resample."}
# print model results
knn_fit2
# plot cross validation results
ggplot(knn_fit2)
```
